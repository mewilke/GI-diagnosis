{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCd60O4rVdxJ"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/datasets/amanik000/gastrointestinal-disease-dataset\n",
        "# This software analyzes a dataset linking 36 features of health data to various\n",
        "# gastrointestinal disease states. The dataset can be found at\n",
        "# https://www.kaggle.com/datasets/amanik000/gastrointestinal-disease-dataset\n",
        "# This software could serve as the basis for a diagnostic tool to guide doctors\n",
        "# in diagnosing gastrointestinal disorders."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Demographic and Physical Attributes**\n",
        "\n",
        "Age: Age of the individual (in years).\n",
        "\n",
        "Gender: Biological gender of the individual (Male/Female/Other).\n",
        "\n",
        "BMI: Body Mass Index – a measure of body fat based on height and weight.\n",
        "\n",
        "Body_Weight: Weight of the individual (in kilograms or pounds).\n",
        "\n",
        "**Health and Obesity Indicators**\n",
        "\n",
        "Obesity_Status: Classification of individual as Obese, Overweight, or Normal based on BMI and other factors.\n",
        "\n",
        "Ethnicity: Ethnic background (e.g., Caucasian, Asian, African American, etc.).\n",
        "\n",
        "Family_History: Indicates whether there is a family history of related diseases (Yes/No).\n",
        "\n",
        "**Biological and Genetic Markers**\n",
        "\n",
        "Genetic_Markers: Presence of known genetic markers associated with disease risk.\n",
        "\n",
        "Microbiome_Index: Quantitative index representing the state of the individual's gut microbiome.\n",
        "\n",
        "**Autoimmune and Inflammatory Indicators**\n",
        "\n",
        "Autoimmune_Disorders: Indicates the presence of known autoimmune disorders (Yes/No).\n",
        "\n",
        "H_Pylori_Status: Status of Helicobacter pylori infection (Positive/Negative).\n",
        "\n",
        "Fecal_Calprotectin: Inflammatory marker found in stool; high levels may indicate IBD.\n",
        "\n",
        "Occult_Blood_Test: Results of fecal occult blood testing (Positive/Negative).\n",
        "\n",
        "CRP_ESR: Combined result of C-Reactive Protein and Erythrocyte Sedimentation Rate – inflammation indicators.\n",
        "\n",
        "**Diagnostic Results**\n",
        "\n",
        "Endoscopy_Result: Findings from endoscopy (e.g., Normal, Inflammation, Ulcers).\n",
        "\n",
        "Colonoscopy_Result: Findings from colonoscopy (e.g., Polyps, IBD, Normal).\n",
        "\n",
        "Stool_Culture: Results indicating presence of bacterial, viral, or parasitic infections.\n",
        "\n",
        "**Lifestyle and Diet**\n",
        "\n",
        "Diet_Type: Type of diet followed (e.g., Vegetarian, Vegan, Omnivore).\n",
        "\n",
        "Food_Intolerance: Known food intolerances (e.g., Lactose, Gluten).\n",
        "\n",
        "Smoking_Status: Current smoking behavior (Smoker/Non-Smoker/Former Smoker).\n",
        "\n",
        "Alcohol_Use: Frequency or status of alcohol consumption.\n",
        "\n",
        "**Mental and Physical Health**\n",
        "\n",
        "Stress_Level: Self-reported stress level (Low/Moderate/High).\n",
        "\n",
        "Physical_Activity: Level of physical activity (Sedentary/Moderate/Active).\n",
        "\n",
        "**Symptoms and GI Issues**\n",
        "\n",
        "Abdominal_Pain: Presence of abdominal pain (Yes/No).\n",
        "\n",
        "Bloating: Whether the individual experiences bloating (Yes/No).\n",
        "\n",
        "Diarrhoea: Presence of diarrhea symptoms (Yes/No).\n",
        "\n",
        "Constipation: Presence of constipation symptoms (Yes/No).\n",
        "\n",
        "Rectal_Bleeding: Observation of rectal bleeding (Yes/No).\n",
        "\n",
        "Appetite_Loss: Decreased appetite (Yes/No).\n",
        "\n",
        "Weight_Loss: Unintentional weight loss (Yes/No).\n",
        "\n",
        "Bowel_Habits: Summary or description of bowel movement characteristics.\n",
        "\n",
        "Bowel_Movement_Frequency: Frequency of bowel movements (per day/week).\n",
        "\n",
        "**Medication Usage**\n",
        "\n",
        "NSAID_Use: Use of Non-Steroidal Anti-Inflammatory Drugs (Yes/No).\n",
        "\n",
        "Antibiotic_Use: Recent use of antibiotics (Yes/No).\n",
        "\n",
        "PPI_Use: Use of Proton Pump Inhibitors (Yes/No).\n",
        "\n",
        "Medications: Other medications currently being taken.\n",
        "\n",
        "**Target Variable**\n",
        "\n",
        "Disease_Class: Final disease classification (e.g., Crohn’s Disease, Ulcerative Colitis, IBS, Healthy)."
      ],
      "metadata": {
        "id": "0Dp6yqQA-4xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from scipy.linalg import svd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, HalvingGridSearchCV, HalvingRandomSearchCV\n",
        "import graphviz as gv\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "IFS0Z1g6WE4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the dataset\n",
        "df = pd.read_csv('./data/gastrointestinal_disease_dataset.csv')\n",
        "num_duplicate_rows = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {num_duplicate_rows}\")"
      ],
      "metadata": {
        "id": "xoz7B5SD1soh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# examine the data\n",
        "df.head()"
      ],
      "metadata": {
        "id": "buqNLSSM13Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "# Note there are no Null fields\n",
        "# BMI (Body Mass Index)\n",
        "# The CRP/ESR ratio is a combination of measures of inflamation.\n",
        "#   High Ratio (> 2): May point towards acute inflammation or bacterial infection.\n",
        "#   Low Ratio (< 1): May suggest chronic inflammation or autoimmune conditions.\n",
        "# PPI_Use (Proton pump inhibitors) are a class of medications that reduce the production of stomach acid."
      ],
      "metadata": {
        "id": "7hql853c2kY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe())"
      ],
      "metadata": {
        "id": "XiWqS7sB23Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.nunique().to_frame(name='nunique'))\n",
        "# Note all of the large numbers are for continuous data.\n",
        "# Disease_Class is categorical but has only 6 categories. It is the target."
      ],
      "metadata": {
        "id": "ytsj2i5MJ2bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Disease_Class'].value_counts()\n",
        "# The target values are balanced."
      ],
      "metadata": {
        "id": "OP5RR6d727Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple view of number of unique values, min value, and max value\n",
        "df_describe = df.describe()\n",
        "df_nunique = df.nunique().to_frame(name='nunique')\n",
        "combined_df = df_describe.T.merge(df_nunique, left_index=True, right_index=True, how='left')\n",
        "print(combined_df[['nunique', 'min', 'max']])"
      ],
      "metadata": {
        "id": "fgFI_B9ZRAns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert columns to numeric values\n",
        "# One-hot encoding\n",
        "df_dummies = pd.get_dummies(df, columns=['Ethnicity', 'Diet_Type', 'Bowel_Habits'], dtype='int8')\n",
        "\n",
        "# Replace strings with numbers\n",
        "df_dummies['Obesity_Status'] = df_dummies['Obesity_Status'].replace({'Underweight':-1, 'Normal':0, 'Overweight':1, 'Obese':2}).astype('int8')\n",
        "df_dummies['Gender'] = df_dummies['Gender'].replace({'Male':0, 'Female':1}).astype('int8')\n",
        "\n",
        "# Define the dtypes for multiple columns to reduce memory usage. The smallest available int is int8.\n",
        "# Continuous features will be squared and cubed later so leaving as int64 or float64\n",
        "dtype_mapping = {\n",
        "    'Family_History': 'int8',\n",
        "    'Autoimmune_Disorders': 'int8',\n",
        "    'H_Pylori_Status': 'int8',\n",
        "    'Occult_Blood_Test': 'int8',\n",
        "    'Endoscopy_Result': 'int8',\n",
        "    'Colonoscopy_Result': 'int8',\n",
        "    'Stool_Culture': 'int8',\n",
        "    'Food_Intolerance': 'int8',\n",
        "    'Smoking_Status': 'int8',\n",
        "    'Alcohol_Use': 'int8',\n",
        "    'Abdominal_Pain': 'int8',\n",
        "    'Bloating': 'int8',\n",
        "    'Diarrhea': 'int8',\n",
        "    'Constipation': 'int8',\n",
        "    'Rectal_Bleeding': 'int8',\n",
        "    'Appetite_Loss': 'int8',\n",
        "    'Weight_Loss': 'int8',\n",
        "    'NSAID_Use': 'int8',\n",
        "    'Antibiotic_Use': 'int8',\n",
        "    'PPI_Use': 'int8',\n",
        "    'Medications': 'int8'\n",
        "}\n",
        "\n",
        "# Apply the dtypes\n",
        "df = df_dummies.astype(dtype_mapping)\n",
        "\n",
        "# Encode the target classes\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['Disease_Class'] = le.fit_transform(df['Disease_Class'])"
      ],
      "metadata": {
        "id": "auCreO_G_2V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "GY7Rs8pbFm6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Disease_Class'].head()\n",
        "#le.inverse_transform(df['Disease_Class'])"
      ],
      "metadata": {
        "id": "j6Vp9DOjvFjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Matrix\n",
        "plt.figure(figsize=(64,64))\n",
        "ax = sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".1f\",  annot_kws={\"size\": 24})\n",
        "plt.title('Correlation Matrix', fontsize=40)\n",
        "ax.tick_params(axis='x', labelsize=20)\n",
        "ax.tick_params(axis='y', labelsize=20)\n",
        "# The correlations of features with the target categories are very low."
      ],
      "metadata": {
        "id": "A_s-6l38dC0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering\n",
        "# There is no effect of squaring or cubing the continuous features on the correlation with the target.\n",
        "# I plotted the correlation matrices and could see no difference.\n",
        "# Then I calculated the matrices and looked at the differences between them. There is none.\n",
        "# I am proceeding with the non-engineered values for the continuous features after showing these calculations.\n",
        "# You can uncomment the code for the plots below to see them.\n",
        "# Temp dataframe\n",
        "feats_eng = df[['Age', 'BMI', 'Body_Weight', 'Bowel_Movement_Frequency', 'CRP_ESR', 'Fecal_Calprotectin', 'Genetic_Markers', 'Microbiome_Index', 'Physical_Activity', 'Stress_Level']].copy()\n",
        "feats_eng2 = feats_eng **2\n",
        "feats_eng3 = feats_eng **3\n",
        "feats_eng1 = pd.concat([feats_eng, df], axis=1)\n",
        "feats_eng2 = pd.concat([feats_eng, df], axis=1)\n",
        "feats_eng3 = pd.concat([feats_eng, df], axis=1)\n",
        "\n",
        "# Correlation Matrix plot\n",
        "#plt.figure(figsize=(16,16))\n",
        "#ax = sns.heatmap(feats_eng1.corr(), annot=True, cmap='coolwarm', fmt=\".2f\",  annot_kws={\"size\": 10})\n",
        "#plt.title('Correlation Matrix', fontsize=30)\n",
        "#ax.tick_params(axis='x', labelsize=10)\n",
        "#ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "feats_eng1.corr() - feats_eng3.corr()"
      ],
      "metadata": {
        "id": "7rpe8FQj7wlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats_eng1.corr() - feats_eng2.corr()"
      ],
      "metadata": {
        "id": "0vI8U_EMMcVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats_eng2.corr() - feats_eng3.corr()"
      ],
      "metadata": {
        "id": "X8vJqJNgMfV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Matrix with squared values\n",
        "#plt.figure(figsize=(16,16))\n",
        "#ax = sns.heatmap(feats_eng2.corr(), annot=True, cmap='coolwarm', fmt=\".2f\",  annot_kws={\"size\": 10})\n",
        "#plt.title('Correlation Matrix with Features Squared', fontsize=30)\n",
        "#ax.tick_params(axis='x', labelsize=10)\n",
        "#ax.tick_params(axis='y', labelsize=10)"
      ],
      "metadata": {
        "id": "FelUWDCIAItl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Matrix with cubed values\n",
        "#plt.figure(figsize=(16,16))\n",
        "#ax = sns.heatmap(feats_eng3.corr(), annot=True, cmap='coolwarm', fmt=\".2f\",  annot_kws={\"size\": 10})\n",
        "#plt.title('Correlation Matrix with Features Cubed', fontsize=30)\n",
        "#ax.tick_params(axis='x', labelsize=10)\n",
        "#ax.tick_params(axis='y', labelsize=10)"
      ],
      "metadata": {
        "id": "l_NnWrvKIosA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obesity_Status and BMI are highly correlated. Dropping Obesity_Status (range -1 - 2)\n",
        "# because BMI has a wider range of values (range 16 - 40).\n",
        "df = df.drop('Obesity_Status', axis=1)\n",
        "\n",
        "# Split the data into train and test datasets\n",
        "X = df.drop('Disease_Class', axis=1)\n",
        "y = df['Disease_Class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Ff1xDQdGFqa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering of disease class using K-means\n",
        "X = np.array(X)\n",
        "inertias = []\n",
        "for i in range(2, 11):\n",
        "  kmeans = KMeans(n_clusters=i, random_state=42).fit(X)\n",
        "  inertias.append(kmeans.inertia_/1000000) # divide by 1000000 to make results easier to interpret\n",
        "print(inertias)"
      ],
      "metadata": {
        "id": "y0rj6xkCMUTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(2, 11), inertias)\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Inertia/1,000,000\")\n",
        "plt.title(\"Inertia vs. Number of Clusters\")\n",
        "# Number of Clusters = 4-5 are reasonable values"
      ],
      "metadata": {
        "id": "zDu-Y6fonIJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering of disease classes using DBSCAN\n",
        "# eps=19.0, min_samples=10 works\n",
        "dbscan = DBSCAN(eps=19.0, min_samples=10).fit(X)\n",
        "labels = dbscan.labels_\n",
        "\n",
        "# Number of clusters in labels, ignoring noise if present.\n",
        "unique_labels = set(labels)\n",
        "n_clusters_ = len(unique_labels) - (1 if -1 in labels else 0)\n",
        "n_noise_ = list(labels).count(-1)\n",
        "\n",
        "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
        "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
        "\n",
        "# Although the Disease Class has 6 categories, it is possible that 4 categories will be more useful."
      ],
      "metadata": {
        "id": "RWD9PBTwLZ_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pricipal Component Analysis using SVD to aid with understanding clusters\n",
        "# Normalize and run PCA using SVD\n",
        "def svd_norm(X):\n",
        "    x_norm =(X - X.mean())/X.std()\n",
        "    U, sigma, VT = svd(x_norm, full_matrices=False)\n",
        "    Sigma = np.diag(sigma)\n",
        "    return U, Sigma, VT\n",
        "U, Sigma, VT = svd_norm(X_train)\n",
        "\n",
        "# Plot x = r and y = Sigma\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(1, len(Sigma) + 1), np.diag(Sigma), 'o-')\n",
        "plt.title('Plot of Singular Values')\n",
        "plt.xlabel('Singular Value Index (r)')\n",
        "plt.ylabel('Singular Value ($\\sigma$)')\n",
        "plt.grid(True)\n",
        "# r is very high. Proceeding with computation to be sure."
      ],
      "metadata": {
        "id": "PPpbasHdmjsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot x = r and y = Sigma\n",
        "SigSum = []\n",
        "for i in range(len(Sigma)):\n",
        "  SigSum.append(np.diag(Sigma)[:i].sum())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(1, len(Sigma) + 1), SigSum/SigSum[-1], 'o-')\n",
        "plt.title('Plot of Singular Values')\n",
        "plt.xlabel('Singular Value Index (r)')\n",
        "plt.ylabel('% Singular Value (%$\\sigma$)')\n",
        "plt.xticks([5, 10, 15, 20, 25, 30, 35, 40, 45])\n",
        "plt.yticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
        "plt.grid(True)\n",
        "\n",
        "# r is about 38 singular values to predict the target with 90% accuracy.\n",
        "# This means virtually all of the features will be needed to categorize these data."
      ],
      "metadata": {
        "id": "uXo13BdRg4F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification\n",
        "# Trying the various classification algorithms.\n",
        "# Note that linear regression is not a good choice because the target has classes not a gradient.\n",
        "\n",
        "# KNN: classification, new data compared to all training data is expensive.\n",
        "# We cannot eliminate more than a few features. Therefore not using KNN.\n",
        "start_time = time.time()\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "print(knn.score(X_test, y_test))\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - start_time} seconds\")\n",
        "# I estimate it will take about 2 hours wall time to tune KNN on this dataset."
      ],
      "metadata": {
        "id": "U10iOVL38xf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression is best for 2 classes. These data have 4 - 6 classes.\n",
        "# Not using Logistic Regression.\n",
        "# Decision Tree\n",
        "params = {\n",
        "    'max_depth': [None, 1, 2, 3],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'min_samples_leaf': [1, 2, 5]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(),\n",
        "                           param_grid=params,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print('GridSearchCV best score: ', grid_search.best_score_)\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# RandomizedSearchCV\n",
        "grid_search = RandomizedSearchCV(estimator=DecisionTreeClassifier(),\n",
        "                           param_distributions=params,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print('RandomizedGridSearchCV best score: ', grid_search.best_score_)\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# HalvingGridSearchCV\n",
        "grid_search = HalvingGridSearchCV(estimator=DecisionTreeClassifier(),\n",
        "                           param_grid=params,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print('HalvingGridSearchCV best score: ', grid_search.best_score_)\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# HalvingRandomSearchCV\n",
        "grid_search = HalvingRandomSearchCV(estimator=DecisionTreeClassifier(),\n",
        "                           param_distributions=params,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print('HalvingRandomSearchCV best score: ', grid_search.best_score_)\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# mean_fit_time was not an issue.\n",
        "# GridSearchCV gave the best score with DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "6-fiY8B3eZ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GridSearchCV running DecisionTreeClassifier to tune min_samples_split.\n",
        "# Note that HalvingRandomSearchCV sometimes gives a better score than\n",
        "# GridSearchCV but not always.\n",
        "params = {\n",
        "    'max_depth': [1],\n",
        "    'min_samples_split': [1, 2, 3, 4],\n",
        "    'criterion': ['entropy'],\n",
        "    'min_samples_leaf': [1]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(),\n",
        "                           param_grid=params,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print('GridSearchCV best score: ', grid_search.best_score_)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "LHXCdg4ZNDyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying decision trees\n",
        "decision_tree_model = DecisionTreeClassifier(criterion='entropy', max_depth=1, min_samples_leaf=1, min_samples_split=2)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "print(decision_tree_model.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "idZWzrsNOBMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_data = tree.export_graphviz(decision_tree_model,\n",
        "                                out_file=None,\n",
        "                                feature_names=X_train.columns,\n",
        "                                class_names=['Blood in stool', 'Abdominal cramps or pain', 'Nausea or vomiting', 'Unexplained weight loss', 'Bloating', 'Diarrhea or constipation'],\n",
        "                                filled=True,\n",
        "                                rounded=True)\n",
        "graph = gv.Source(dot_data)\n",
        "graph.render(format=\"png\", filename=\"GI_tree\")\n",
        "graph\n",
        "# This graph has the best accuracy score but is not useful for diagnosisng patients.\n",
        "# It fails to diagnose the other 4 categories of the target class.\n",
        "# Based on the clustering, we expect 4 categories. This will require 2 splits.\n",
        "# Based on the number of target categories, we expect 3 splits.\n",
        "# Trying 2 and 3 splits next."
      ],
      "metadata": {
        "id": "FZKxRkvxOOE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 splits\n",
        "decision_tree_model = DecisionTreeClassifier(criterion='entropy', max_depth=2, min_samples_leaf=1, min_samples_split=2)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "print(decision_tree_model.score(X_test, y_test))\n",
        "# This gives leaves of 4 of the 6 classes."
      ],
      "metadata": {
        "id": "mqqorA27QV1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_data = tree.export_graphviz(decision_tree_model,\n",
        "                                out_file=None,\n",
        "                                feature_names=X_train.columns,\n",
        "                                class_names=['Blood in stool', 'Abdominal cramps or pain', 'Nausea or vomiting', 'Unexplained weight loss', 'Bloating', 'Diarrhea or constipation'],\n",
        "                                filled=True,\n",
        "                                rounded=True)\n",
        "graph = gv.Source(dot_data)\n",
        "graph.render(format=\"png\", filename=\"GI_tree\")\n",
        "graph"
      ],
      "metadata": {
        "id": "bwkgDXvzQ6KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 splits\n",
        "decision_tree_model = DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_leaf=1, min_samples_split=2)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "print(decision_tree_model.score(X_test, y_test))\n",
        "# This gives leaves of 5 of the 6 classes. The numbers are not balanced but are balanced in X."
      ],
      "metadata": {
        "id": "R57R3MANSynG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_data = tree.export_graphviz(decision_tree_model,\n",
        "                                out_file=None,\n",
        "                                feature_names=X_train.columns,\n",
        "                                class_names=['Blood in stool', 'Abdominal cramps or pain', 'Nausea or vomiting', 'Unexplained weight loss', 'Bloating', 'Diarrhea or constipation'],\n",
        "                                filled=True,\n",
        "                                rounded=True)\n",
        "graph = gv.Source(dot_data)\n",
        "graph.render(format=\"png\", filename=\"GI_tree\")\n",
        "graph"
      ],
      "metadata": {
        "id": "aYX0aHrXTDDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HalvingRandomSearchCV running DecisionTreeClassifier to tune min_samples_split.\n",
        "# Note that HalvingRandomSearchCV sometimes gives a better score than\n",
        "# GridSearchCV but not always.\n",
        "params = {\n",
        "    'max_depth': [None],\n",
        "    'min_samples_split': [6,7,8,9,10,11,12,13,14,15],\n",
        "    'criterion': ['entropy'],\n",
        "    'min_samples_leaf': [2,3,4]\n",
        "}\n",
        "\n",
        "# HalvingRandomSearchCV\n",
        "grid_search = HalvingRandomSearchCV(estimator=DecisionTreeClassifier(),\n",
        "                           param_distributions=params,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print('GridSearchCV best score: ', grid_search.best_score_)\n",
        "print(grid_search.best_params_)\n",
        "# Ran repeatedly. Best result was\n",
        "# GridSearchCV best score:  0.1761399686580285\n",
        "# {'min_samples_split': 6, 'min_samples_leaf': 3, 'max_depth': None, 'criterion': 'entropy'}"
      ],
      "metadata": {
        "id": "UOteu6oQcL2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree_model = DecisionTreeClassifier(criterion='entropy', max_depth=None, min_samples_leaf=3, min_samples_split=6)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "print(decision_tree_model.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "ngA55kUXfWLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Produces a huge tree. Run at your own risk!\n",
        "#dot_data = tree.export_graphviz(decision_tree_model,\n",
        "#                                out_file=None,\n",
        "#                                feature_names=X_train.columns,\n",
        "#                                class_names=['Blood in stool', 'Abdominal cramps or pain', 'Nausea or vomiting', 'Unexplained weight loss', 'Bloating', 'Diarrhea or constipation'],\n",
        "#                                filled=True,\n",
        "#                                rounded=True)\n",
        "#graph = gv.Source(dot_data)\n",
        "#graph.render(format=\"png\", filename=\"GI_tree\")\n",
        "#graph"
      ],
      "metadata": {
        "id": "wzGHDjoqfxI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GraphViz version is from 2006: \", gv.version())\n",
        "# There is a very old version of GraphViz on Colab. I was unable to find documentation for it. I wanted to change the size of the image.\n",
        "# The size of of an unlimited depth decision tree is non-functional for these data.\n",
        "# Because there are a large number of weak features bagging and specifically Random Forest are good algorithms to try."
      ],
      "metadata": {
        "id": "pw_F9vqgdyGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "9xr2FiJh3wpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "# Using all 45 columns as max_features because almost all needed to reach 90% accuracy.\n",
        "# Takes 10 minutes wall time to run\n",
        "model = RandomForestClassifier(n_estimators=1000, max_features=45, oob_score=True)\n",
        "model.fit(X, y)\n",
        "print(model.oob_score_)\n",
        "# Still getting only a very weak signal out of these data."
      ],
      "metadata": {
        "id": "INqTiByTfuUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning max_features for RandomForestClassifier\n",
        "# Takes about 15 minutes to run\n",
        "#params = {'max_features': [45, 44, 43, 42, 41, 40],\n",
        "#          'n_estimators': [100],\n",
        "#          'oob_score': [True]}\n",
        "#grid_search = GridSearchCV(estimator=RandomForestClassifier(),\n",
        "#                           param_grid=params,\n",
        "#                           cv=5,\n",
        "#                           scoring='accuracy')\n",
        "#grid_search.fit(X_train, y_train)\n",
        "#print('GridSearchCV best score: ', grid_search.best_score_)\n",
        "#print(grid_search.best_params_)\n",
        "\n",
        "# Output\n",
        "# GridSearchCV best score:  0.16737558251255585\n",
        "# {'max_features': 44, 'n_estimators': 100, 'oob_score': True}"
      ],
      "metadata": {
        "id": "FkeK7Qhn3Ynf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running RandomForestClassifier with max_features tuned\n",
        "# Takes about 6 minutes wall time to run\n",
        "model = RandomForestClassifier(n_estimators=1000, max_features=44, oob_score=True)\n",
        "model.fit(X, y)\n",
        "print(model.oob_score_)"
      ],
      "metadata": {
        "id": "EfPMp-VS9j3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pairplot out of desperation. This plots the continuous variables to see if any relationships jump out.\n",
        "# I don't see any reason to use SVM to separate clusters because I don't see any complex relationships.\n",
        "sns.pairplot(feats_eng)\n",
        "# Apart from the expected relationship between weight and BMI, no relatioinships are visible.\n",
        "# This is not surprising considering the correlation coefficients."
      ],
      "metadata": {
        "id": "FTkizCAcVBqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression\n",
        "lin_reg = LinearRegression(fit_intercept=False).fit(X, y)\n",
        "y_pred = lin_reg.predict(X_test)\n",
        "print('Mean Squared Error: ', mean_squared_error(y_test, y_pred))\n",
        "# Meh"
      ],
      "metadata": {
        "id": "KCpY4v1bjLby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes\n",
        "naive_bayes = MultinomialNB(alpha=1.0, fit_prior=False).fit(X_train, y_train)\n",
        "print(naive_bayes.feature_log_prob_)\n",
        "# This is too simple for the given problem.\n",
        "# The numbers for each disease type (row) indicate which feature is most likely to be the cause\n",
        "# The problem is we know that it will take almost all the features to predict the disease type.\n",
        "# This is predicting the feature from the disease type instead of the disease type from the combined features."
      ],
      "metadata": {
        "id": "zrFOQnlSn5ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Nets"
      ],
      "metadata": {
        "id": "ky57GkNzClEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent"
      ],
      "metadata": {
        "id": "IVXtsQUM8ZAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate all the different models"
      ],
      "metadata": {
        "id": "rKSrm_Yz69b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP?"
      ],
      "metadata": {
        "id": "fb6MtEYI8-N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommendation System?"
      ],
      "metadata": {
        "id": "onA3md659Tli"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}